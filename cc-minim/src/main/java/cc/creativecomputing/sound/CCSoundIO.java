/*
 *  Copyright (c) 2007 - 2008 by Damien Di Fede <ddf@compartmental.net>
 *
 *   This program is free software; you can redistribute it and/or modify
 *   it under the terms of the GNU Library General Public License as published
 *   by the Free Software Foundation; either version 2 of the License, or
 *   (at your option) any later version.
 *
 *   This program is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU Library General Public License for more details.
 *
 *   You should have received a copy of the GNU Library General Public
 *   License along with this program; if not, write to the Free Software
 *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

package cc.creativecomputing.sound;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.MalformedURLException;
import java.net.URL;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

import javax.sound.sampled.AudioFileFormat;
import javax.sound.sampled.AudioFormat;
import javax.sound.sampled.AudioInputStream;
import javax.sound.sampled.AudioSystem;
import javax.sound.sampled.DataLine;
import javax.sound.sampled.LineUnavailableException;
import javax.sound.sampled.Mixer;
import javax.sound.sampled.SourceDataLine;
import javax.sound.sampled.TargetDataLine;
import javax.sound.sampled.UnsupportedAudioFileException;

import javazoom.spi.mpeg.sampled.file.MpegAudioFormat;

import org.tritonus.share.sampled.AudioUtils;
import org.tritonus.share.sampled.file.TAudioFileFormat;

import cc.creativecomputing.io.CCNIOUtil;

/**
 * The <code>{@linkplain #CCSoundIO()}</code> class is the starting point for most everything you
 * will do with this library. There are methods for obtaining objects for
 * playing audio files: {@linkplain #CCAudioSample} and AudioPlayer. There are methods for
 * obtaining an AudioRecorder, which is how you record audio to disk. There are
 * methods for obtaining an AudioInput, which is how you can monitor the
 * computer's line-in or microphone, depending on what the user has set as the
 * record source. Finally there are methods for obtaining an AudioOutput, which
 * is how you can play audio generated by your program, typically by connecting
 * classes found in the ugens package.
 * <p>
 * Minim keeps references to all of the resources that are returned from these
 * various methods so that you don't have to worry about closing them. Instead,
 * when your application ends you can simply call the stop method of your Minim
 * instance. Processing users <em>do not</em> need to do this because Minim
 * detects when a PApplet is passed to the contructor and registers for a
 * notification of application shutdown.
 * <p>
 * Minim requires an Object that can handle two important file system operations
 * so that it doesn't have to worry about details of the current environment.
 * These two methods are:
 * <p>
 * <code>
 * String sketchPath( String fileName )<br/>
 * InputStream createInput( String fileName )<br/>
 * </code>
 * </p>
 * These are methods that are defined in Processing, which Minim was originally
 * designed to cleanly interface with. The <code>sketchPath</code> method is
 * expected to transform a filename into an absolute path and is used when
 * attempting to create an AudioRecorder. The <code>createInput</code> method is
 * used when loading files and is expected to take a filename, which is not
 * necessarily an absolute path, and return an <code>InputStream</code> that can
 * be used to read the file. For example, in Processing, the
 * <code>createInput</code> method will search in the data folder, the sketch
 * folder, handle URLs, and absolute paths. If you are using Minim outside of
 * Processing, you can handle whatever cases are appropriate for your project.
 * 
 * @example Basics/PlayAFile
 * 
 * @author Damien Di Fede
 */

public class CCSoundIO {
	/** Specifies that you want a MONO AudioInput or AudioOutput */
	public static final int MONO = 1;
	/** Specifies that you want a STEREO AudioInput or AudioOutput */
	public static final int STEREO = 2;

	public static final int LOOP_CONTINUOUSLY = -1;

	private static boolean DEBUG = false;
	
	private static CCSoundIO instance;
	
	public static CCSoundIO instance(){
		if(instance != null)return instance;
		return instance = new CCSoundIO();
	}

	// we keep track of all the resources we are asked to create
	// so that when shutting down the library, users can simply call stop(),
	// and don't have to call close() on all of the things they've created.
	// in the event that they *do* call close() on resource we've created,
	// it will be removed from this list.
	private static ArrayList<CCAudioResource> _myResources = new ArrayList<CCAudioResource>();
	// and unfortunately we have to track stream separately
	private static ArrayList<CCAudioStream> streams = new ArrayList<CCAudioStream>();

	private static Mixer inputMixer;
	private static Mixer outputMixer;

	/**
	 * Creates an instance of CCSoundIO.
	 * 
	 */
	public CCSoundIO() {
		inputMixer = null;
		outputMixer = null;

	}

	/**
	 * @invisible
	 * 
	 *            Displays a debug message, but only if {@link #debugOn()} has
	 *            been called. The message will be displayed in the console area
	 *            of the PDE, if you are running your sketch from the PDE.
	 *            Otherwise, it will be displayed in the Java Console.
	 * 
	 * @param message the message to display
	 * @see #debugOn()
	 */
	public static void debug(String message) {
		if (DEBUG) {
			String[] lines = message.split("\n");
			System.out.println("=== Minim Debug ===");
			for (int i = 0; i < lines.length; i++) {
				System.out.println("=== " + lines[i]);
			}
			System.out.println();
		}
	}

	/**
	 * Turns on debug messages.
	 */
	public static void debugOn() {
		DEBUG = true;
	}

	/**
	 * Turns off debug messages.
	 * 
	 */
	public static void debugOff() {
		DEBUG = false;
	}

	/**
	 * @invisible
	 * 
	 *            Library callback used by Processing when a sketch is being
	 *            shutdown. It is not necessary to call this directly. It simply
	 *            calls stop().
	 * 
	 * 
	 */
	public void dispose() {
		stop();
	}

	/**
	 * 
	 * Stops Minim and releases all audio resources.
	 * <p>
	 * If using Minim outside of Processing, you must call this to release all
	 * of the audio resources that Minim has generated. It will call close() on
	 * all of them for you.
	 * 
	 */
	public void stop() {
		debug("Stopping Minim...");

		// close all sources and release them
		for (CCAudioResource s : _myResources) {
			// null the parent so the AudioSource doesn't try to call
			// removeSource
			s.close();
		}
		_myResources.clear();

		for (CCAudioStream s : streams) {
			s.close();
		}

	}

	static void addSource(CCAudioResource s) {
		_myResources.add(s);
	}

	static void removeSource(CCAudioResource s) {
		_myResources.remove(s);
	}

	/**
	 * When using the JavaSound implementation of Minim, this sets the JavaSound
	 * Mixer that will be used for obtaining input sources such as AudioInputs.
	 * THIS METHOD WILL BE REPLACED IN A FUTURE VERSION.
	 * 
	 * @param mixer The Mixer we should try to acquire inputs from.
	 */
	public static void setInputMixer(Mixer mixer) {
		inputMixer = mixer;
	}

	/**
	 * Creates an AudioSample using the provided sample data and AudioFormat.
	 * When a buffer size is not provided, it defaults to 1024. The buffer size
	 * of a sample controls the size of the left, right, and mix AudioBuffer
	 * fields of the returned AudioSample.
	 * 
	 * @param sampleData float[]: the single channel of sample data
	 * @param format the AudioFormat describing the sample data
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 */
	public static CCAudioSample createSample(float[] sampleData, AudioFormat format) {
		return createSample(sampleData, format, 1024);
	}

	static SourceDataLine getSourceDataLine(AudioFormat format, int bufferSize) {
		SourceDataLine line = null;
		DataLine.Info info = new DataLine.Info(SourceDataLine.class, format);
		if (AudioSystem.isLineSupported(info)) {
			try {
				if (outputMixer == null) {
					line = (SourceDataLine) AudioSystem.getLine(info);
				} else {
					line = (SourceDataLine) outputMixer.getLine(info);
				}
				// remember that time you spent, like, an entire afternoon
				// fussing
				// with this buffer size to try to get the latency decent on
				// Linux?
				// Yah, don't fuss with this anymore, ok?
				line.open(format, bufferSize * format.getFrameSize() * 4);
				if (line.isOpen()) {
					debug("SourceDataLine is " + line.getClass().toString() + "\n" + "Buffer size is "
							+ line.getBufferSize() + " bytes.\n" + "Format is " + line.getFormat().toString() + ".");
					return line;
				}
			} catch (LineUnavailableException e) {
				throw new CCSoundException("Couldn't open the line", e);
			}
		}
		throw new CCSoundException("Unable to return a SourceDataLine: unsupported format - " + format.toString());
	}

	/**
	 * Should return an {@link CCAudioOutput} that can be used to generate audio
	 * that will be heard through the computer's speakers.
	 * 
	 * @param type Minim.STEREO or Minim.MONO
	 * @param bufferSize how big the in-memory buffer should be
	 * @param sampleRate what the sample rate of the generated audio should be
	 * @param bitDepth what the bit depth of the generated audio should be
	 * @return an AudioSynthesizer that will output to the computer's speakers
	 */
	static public CCAudioOutput getAudioOutput(int type, int bufferSize, float sampleRate, int bitDepth) {
		if (bitDepth != 8 && bitDepth != 16) {
			throw new IllegalArgumentException("Unsupported bit depth, use either 8 or 16.");
		}
		AudioFormat format = new AudioFormat(sampleRate, bitDepth, type, true, false);
		SourceDataLine sdl = getSourceDataLine(format, bufferSize);
		if (sdl != null) {
			return new CCAudioOutput(sdl, bufferSize);
		}
		return null;
	}

	private static CCAudioSample getAudioSampleImp(CCFloatSampleBuffer samples, AudioFormat format, int bufferSize) {
		CCAudioOutput out = getAudioOutput(samples.getChannelCount(), bufferSize, format.getSampleRate(), format.getSampleSizeInBits());
		
		if (out != null) {
			CCSampleSignal ssig = new CCSampleSignal(samples);
			out.setAudioSignal(ssig);
			long length = AudioUtils.frames2Millis(samples.getSampleCount(), format);
			CCAudioMetaData meta = new CCAudioMetaData(Paths.get(samples.toString()), length, samples.getSampleCount());
			return new CCAudioSample(meta, ssig, out);
		} else {
			throw new CCSoundException("Couldn't acquire an output.");
		}
	}

	/**
	 * Should return an {@link CCAudioSample} that will store the provided
	 * samples.
	 * 
	 * @param samples the array of audio samples
	 * @param bufferSize how large the output buffer should be
	 * @return an AudioSample that contains the samples
	 */
	public static CCAudioSample getAudioSample(float[] samples, AudioFormat format, int bufferSize) {
		CCFloatSampleBuffer sample = new CCFloatSampleBuffer(1, samples.length, format.getSampleRate());
		System.arraycopy(samples, 0, sample.getChannel(0), 0, samples.length);
		return getAudioSampleImp(sample, format, bufferSize);
	}

	/**
	 * Creates an AudioSample using the provided sample data and AudioFormat,
	 * with the desired output buffer size.
	 * 
	 * @param sampleData float[]: the single channel of sample data
	 * @param format the AudioFormat describing the sample data
	 * @param bufferSize int: the output buffer size to use, which controls the
	 *            size of the left, right, and mix AudioBuffer fields of the
	 *            returned AudioSample.
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 */
	public static CCAudioSample createSample(float[] sampleData, AudioFormat format, int bufferSize) {
		CCAudioSample sample = getAudioSample(sampleData, format, bufferSize);
		addSource(sample);
		return sample;
	}

	/**
	 * Creates an AudioSample using the provided left and right channel sample
	 * data with an output buffer size of 1024.
	 * 
	 * @param leftSampleData float[]: the left channel of the sample data
	 * @param rightSampleData float[]: the right channel of the sample data
	 * @param format the AudioFormat describing the sample data
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 */
	public static CCAudioSample createSample(float[] leftSampleData, float[] rightSampleData, AudioFormat format) {
		return createSample(leftSampleData, rightSampleData, format, 1024);
	}

	/**
	 * Should return an {@link CCAudioSample} that will store the provided
	 * samples.
	 * 
	 * @param left the left channel of the stereo sample
	 * @param right the right channel of a stereo sample
	 * @param bufferSize how large the output buffer should be
	 * @return an AudioSample that contains the samples
	 */
	public static CCAudioSample getAudioSample(float[] left, float[] right, AudioFormat format, int bufferSize) {
		CCFloatSampleBuffer sample = new CCFloatSampleBuffer(2, left.length, format.getSampleRate());
		System.arraycopy(left, 0, sample.getChannel(0), 0, left.length);
		System.arraycopy(right, 0, sample.getChannel(1), 0, right.length);
		return getAudioSampleImp(sample, format, bufferSize);
	}

	/**
	 * Creates an AudioSample using the provided left and right channel sample
	 * data.
	 * 
	 * @param leftSampleData float[]: the left channel of the sample data
	 * @param rightSampleData float[]: the right channel of the sample data
	 * @param format the AudioFormat describing the sample data
	 * @param bufferSize int: the output buffer size to use, which controls the
	 *            size of the left, right, and mix AudioBuffer fields of the
	 *            returned AudioSample.
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 */
	public static CCAudioSample createSample(float[] leftSampleData, float[] rightSampleData, AudioFormat format, int bufferSize) {
		CCAudioSample sample = getAudioSample(leftSampleData, rightSampleData, format, bufferSize);
		addSource(sample);
		return sample;
	}

	/**
	 * Loads the requested file into an AudioSample. By default, the buffer size
	 * used is 1024.
	 * 
	 * 
	 * @param filename the file or URL that you want to load
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 * 
	 * @see #loadSample(String, int)
	 * @see CCAudioSample
	 * @related AudioSample
	 */
	static public CCAudioSample loadSample(Path filename) {
		return loadSample(filename, 1024);
	}

	/**
	 * This method is a replacement for
	 * AudioSystem.getAudioInputStream(InputStream), which includes workaround
	 * for getting an mp3 AudioInputStream when sketch is running in an applet.
	 * The workaround was developed by the Tritonus team and originally comes
	 * from the package javazoom.jlgui.basicplayer
	 * 
	 * @param is The stream to convert to an AudioInputStream
	 * @return an AudioInputStream that will read from is
	 * @throws UnsupportedAudioFileException
	 * @throws IOException
	 */
	static AudioInputStream getAudioInputStream(InputStream is) throws UnsupportedAudioFileException, IOException {
		try {
			return AudioSystem.getAudioInputStream(is);
		} catch (Exception iae) {
			debug("Using AppletMpegSPIWorkaround to get codec");
			return new CCMpegAudioFileReader().getAudioInputStream(is);
		}
	}
	
	/**
	 * This method is also part of AppletMpegSPIWorkaround, which uses yet
	 * another workaround to load an internet radio stream.
	 * 
	 * @param url the URL of the stream
	 * @return an AudioInputStream of the streaming audio
	 * @throws UnsupportedAudioFileException
	 * @throws IOException
	 */
	static AudioInputStream getAudioInputStream(URL url) {
		try {
			return new CCMpegAudioFileReaderWorkaround().getAudioInputStream(url, null);
		} catch (MalformedURLException e) {
			throw new CCSoundException("Bad URL", e);
		} catch (UnsupportedAudioFileException e) {
			throw new CCSoundException("URL is in an unsupported audio file format", e);
		} catch (IOException e) {
			throw new CCSoundException("Error reading the URL", e);
		}

	}

	static AudioInputStream getAudioInputStream(Path thePath) {
		AudioInputStream ais = null;
		BufferedInputStream bis = null;
		try {
			InputStream is = Files.newInputStream(thePath);
			debug("Base input stream is: " + is.toString());
			bis = new BufferedInputStream(is);
			ais = getAudioInputStream(bis);
			// don't mark it like this because it means the entire
			// file will be loaded into memory as it plays. this
			// will cause out-of-memory problems with very large files.
			// ais.mark((int)ais.available());
			debug("Acquired AudioInputStream.\n" + "It is " + ais.getFrameLength() + " frames long.\n"
					+ "Marking support: " + ais.markSupported());
		} catch (IOException ioe) {
			throw new CCSoundException("IOException", ioe);
		} catch (UnsupportedAudioFileException uafe) {
			throw new CCSoundException("Unsupported Audio File", uafe);
		} catch (Exception e) {
			throw new CCSoundException("Error invoking createInput on the file loader object", e);
		}
		return ais;
	}

	/**
	 * This method is a replacement for
	 * AudioSystem.getAudioInputStream(AudioFormat, AudioInputStream), which is
	 * used for audio format conversion at the stream level. This method
	 * includes a workaround for converting from an mp3 AudioInputStream when
	 * the sketch is running in an applet. The workaround was developed by the
	 * Tritonus team and originally comes from the package
	 * javazoom.jlgui.basicplayer
	 * 
	 * @param targetFormat the AudioFormat to convert the stream to
	 * @param sourceStream the stream containing the unconverted audio
	 * @return an AudioInputStream in the target format
	 */
	static AudioInputStream getAudioInputStream(AudioFormat targetFormat, AudioInputStream sourceStream) {
		try {
			return AudioSystem.getAudioInputStream(targetFormat, sourceStream);
		} catch (IllegalArgumentException iae) {
			debug("Using AppletMpegSPIWorkaround to get codec");
			try {
				Class.forName("javazoom.spi.mpeg.sampled.convert.MpegFormatConversionProvider");
				return new javazoom.spi.mpeg.sampled.convert.MpegFormatConversionProvider().getAudioInputStream(
						targetFormat, sourceStream);
			} catch (ClassNotFoundException cnfe) {
				throw new IllegalArgumentException("Mpeg codec not properly installed");
			}
		}
	}

	static private Map<String, Object> getID3Tags(Path thePath) {
		debug("Getting the properties.");
		Map<String, Object> props = new HashMap<String, Object>();
		try {
			CCMpegAudioFileReader reader = new CCMpegAudioFileReader();
			InputStream stream = Files.newInputStream(thePath);
			AudioFileFormat baseFileFormat = reader.getAudioFileFormat(stream, stream.available());
			stream.close();
			if (baseFileFormat instanceof TAudioFileFormat) {
				TAudioFileFormat fileFormat = (TAudioFileFormat) baseFileFormat;
				props = fileFormat.properties();
				if (props.size() == 0) {
					throw new CCSoundException("No file properties available for " + thePath + ".");
				} else {
					debug("File properties: " + props.toString());
				}
			}
		} catch (UnsupportedAudioFileException e) {
			throw new CCSoundException("Couldn't get the file format for " + thePath, e);
		} catch (IOException e) {
			throw new CCSoundException("Couldn't access " + thePath, e);
		} catch (Exception e) {
			throw new CCSoundException("Error invoking createInput on the file loader object: " + e);
		}

		return props;
	}

	static private CCFloatSampleBuffer loadFloatAudio(AudioInputStream ais, int toRead) {
		CCFloatSampleBuffer samples = new CCFloatSampleBuffer();
		int totalRead = 0;
		byte[] rawBytes = new byte[toRead];
		try {
			// we have to read in chunks because the decoded stream won't
			// read more than about 2000 bytes at a time
			while (totalRead < toRead) {
				int actualRead = ais.read(rawBytes, totalRead, toRead - totalRead);
				if (actualRead < 1) {
					break;
				}
				totalRead += actualRead;
			}
			ais.close();
		} catch (Exception ioe) {
			throw new CCSoundException("Error loading file into memory", ioe);
		}
		debug("Needed to read " + toRead + " actually read " + totalRead);
		samples.initFromByteArray(rawBytes, 0, totalRead, ais.getFormat());
		return samples;
	}

	/**
	 * Should return an {@link CCAudioSample} that will load the requested file
	 * into memory.
	 * 
	 * @param thePath the name of the file to load, this might be a URL, an
	 *            absolute path, or a file that the user expects the
	 *            implementation to find in their sketch somewhere.
	 * @param bufferSize how big the output buffer used for playing the sample
	 *            should be
	 * @return an AudioSample that contains the file
	 */
	static public CCAudioSample getAudioSample(Path thePath, int bufferSize) {
		AudioInputStream ais = getAudioInputStream(thePath);
		if (ais != null) {
			CCAudioMetaData meta = null;
			AudioFormat format = ais.getFormat();
			CCFloatSampleBuffer samples = null;
			if (format instanceof MpegAudioFormat) {
				AudioFormat baseFormat = format;
				format = new AudioFormat(AudioFormat.Encoding.PCM_SIGNED, baseFormat.getSampleRate(), 16,
						baseFormat.getChannels(), baseFormat.getChannels() * 2, baseFormat.getSampleRate(), false);
				// converts the stream to PCM audio from mp3 audio
				ais = getAudioInputStream(format, ais);
				// get a map of properties so we can find out how long it is
				Map<String, Object> props = getID3Tags(thePath);
				// there is a property called mp3.length.bytes, but that is
				// the length in bytes of the mp3 file, which will of course
				// be much shorter than the decoded version. so we use the
				// duration of the file to figure out how many bytes the
				// decoded file will be.
				long dur = ((Long) props.get("duration")).longValue();
				int toRead = (int) AudioUtils.millis2Bytes(dur / 1000, format);
				samples = loadFloatAudio(ais, toRead);
				meta = new CCMP3MetaData(thePath, dur / 1000, props);
			} else {
				samples = loadFloatAudio(ais, (int) ais.getFrameLength() * format.getFrameSize());
				long length = AudioUtils.frames2Millis(samples.getSampleCount(), format);
				meta = new CCAudioMetaData(thePath, length, samples.getSampleCount());
			}
			CCAudioOutput out = getAudioOutput(format.getChannels(), bufferSize, format.getSampleRate(),
					format.getSampleSizeInBits());
			if (out != null) {
				CCSampleSignal ssig = new CCSampleSignal(samples);
				out.setAudioSignal(ssig);
				return new CCAudioSample(meta, ssig, out);
			} else {
				throw new CCSoundException("Couldn't acquire an output.");
			}
		}
		return null;
	}

	/**
	 * Loads the requested file into an AudioSample.
	 * 
	 * @param filename the file or URL that you want to load
	 * @param bufferSize int: The sample buffer size you want. This controls the
	 *            size of the left, right, and mix AudioBuffer fields of the
	 *            returned AudioSample.
	 * 
	 * @return an AudioSample that can be triggered to make sound
	 */
	static public CCAudioSample loadSample(Path filename, int bufferSize) {
		CCAudioSample sample = getAudioSample(filename, bufferSize);
		addSource(sample);
		return sample;
	}

	/**
	 * Loads the requested file into an AudioPlayer. The default buffer size is
	 * 1024 samples and the buffer size determines the size of the left, right,
	 * and mix AudioBuffer fields on the returned AudioPlayer.
	 * 
	 * @param filename the file or URL you want to load
	 * @return an <code>AudioPlayer</code> that plays the file
	 * 
	 * @see #loadFile(String, int)
	 */
	public static CCAudioPlayer loadFile(Path filename) {
		return loadFile(filename, 1024);
	}

	/**
	 * Should return an {@link CCAudioRecordingStream} that will stream the file
	 * requested. The filename could be a URL, an absolute path, or just a
	 * filename that the user expects the system to find in their sketch
	 * somewhere.
	 * 
	 * @param thePath the name of the file to load into the CCAudioRecordingStream
	 * @param bufferSize the bufferSize to use in memory (implementations are
	 *            free to ignore this, if they must)
	 * @param inMemory TODO figure out if this inMemory thing really makes
	 *            sense.
	 * @return an AudioRecording stream that will stream the file
	 */
	public static CCAudioRecordingStream getAudioRecordingStream(Path thePath, int bufferSize, boolean inMemory) {
		// TODO: deal with the case of wanting to have the file fully in memory
		CCAudioRecordingStream mstream = null;
		AudioInputStream ais = getAudioInputStream(thePath);
		if (inMemory && ais.markSupported()) {
			ais.mark((int) ais.getFrameLength() * ais.getFormat().getFrameSize());
		}
		debug("Reading from " + ais.getClass().toString());
		if (ais != null) {
			debug("File format is: " + ais.getFormat().toString());
			AudioFormat format = ais.getFormat();
			// special handling for mp3 files because
			// they need to be converted to PCM
			if (format instanceof MpegAudioFormat) {
				AudioFormat baseFormat = format;
				format = new AudioFormat(AudioFormat.Encoding.PCM_SIGNED, baseFormat.getSampleRate(), 16,
						baseFormat.getChannels(), baseFormat.getChannels() * 2, baseFormat.getSampleRate(), false);
				// converts the stream to PCM audio from mp3 audio
				AudioInputStream decAis = getAudioInputStream(format, ais);
				// source data line is for sending the file audio out to the
				// speakers
				SourceDataLine line = getSourceDataLine(format, bufferSize);
				if (decAis != null && line != null) {
					Map<String, Object> props = getID3Tags(thePath);
					long lengthInMillis = -1;
					if (props.containsKey("duration")) {
						Long dur = (Long) props.get("duration");
						if (dur.longValue() > 0) {
							lengthInMillis = dur.longValue() / 1000;
						}
					}
					CCMP3MetaData meta = new CCMP3MetaData(thePath, lengthInMillis, props);
					mstream = new CCMPEGAudioRecordingStream(meta, ais, decAis, line, bufferSize);
				}
			} // format instanceof MpegAudioFormat
			else {
				// source data line is for sending the file audio out to the
				// speakers
				SourceDataLine line = getSourceDataLine(format, bufferSize);
				if (line != null) {
					long length = AudioUtils.frames2Millis(ais.getFrameLength(), format);
					CCAudioMetaData meta = new CCAudioMetaData(thePath, length, ais.getFrameLength());
					mstream = new CCAudioRecordingStream(meta, ais, line, bufferSize, meta.length());
				}
			} // else
		} // ais != null
		return mstream;
	}

	/**
	 * Loads the requested file into an {@link CCAudioPlayer} with the request
	 * buffer size.
	 * 
	 * @param filename the file or URL you want to load
	 * @param bufferSize int: the sample buffer size you want, which determines
	 *            the size of the left, right, and mix AudioBuffer fields of the
	 *            returned AudioPlayer.
	 * 
	 * @return an <code>AudioPlayer</code> with a sample buffer of the requested
	 *         size
	 */
	public static CCAudioPlayer loadFile(Path filename, int bufferSize) {
		CCAudioPlayer player = null;
		CCAudioRecordingStream rec = getAudioRecordingStream(filename, bufferSize, false);
		if (rec != null) {
			AudioFormat format = rec.getFormat();
			CCAudioOutput out = getAudioOutput(format.getChannels(), bufferSize, format.getSampleRate(),
					format.getSampleSizeInBits());

			if (out != null) {
				player = new CCAudioPlayer(rec, out);
			} else {
				rec.close();
			}
		}

		if (player != null) {
			addSource(player);
		} else {
			throw new CCSoundException("Couldn't load the file " + filename);
		}

		return player;
	}

	/**
	 * Loads the file into an CCAudioRecordingStream, which allows you to stream
	 * audio data from the file yourself. Note that doing this will not result
	 * in any sound coming out of your speakers, unless of course you send it
	 * there. You would primarily use this to perform offline-analysis of a file
	 * or for very custom sound streaming schemes.
	 * 
	 * @param filename the file to load
	 * @param bufferSize int: the bufferSize to use, which controls how much of
	 *            the streamed file is stored in memory at a time.
	 * @param inMemory boolean: whether or not the file should be cached in
	 *            memory as it is read
	 * 
	 * @return an CCAudioRecordingStream that you can use to read from the file.
	 * 
	 * 
	 */
	public static CCAudioRecordingStream loadFileStream(Path filename, int bufferSize, boolean inMemory) {
		CCAudioRecordingStream stream = getAudioRecordingStream(filename, bufferSize, inMemory);
		streams.add(stream);
		return stream;
	}

	/**
	 * Loads the requested file into a MultiChannelBuffer. The buffer's channel
	 * count and buffer size will be adjusted to match the file.
	 * 
	 * @param filename the file to load
	 * @param outBuffer the MultiChannelBuffer to fill with the file's audio
	 *            samples
	 * 
	 * @return the sample rate of audio samples in outBuffer, or 0 if the load
	 *         failed.
	 */
	public static float loadFileIntoBuffer(Path filename, CCMultiChannelBuffer outBuffer) {
		final int readBufferSize = 4096;
		float sampleRate = 0;
		CCAudioRecordingStream stream = getAudioRecordingStream(filename, readBufferSize, false);
		if (stream != null) {
			// stream.open();
			stream.play();
			sampleRate = stream.getFormat().getSampleRate();
			final int channelCount = stream.getFormat().getChannels();
			// for reading the file in, in chunks.
			CCMultiChannelBuffer readBuffer = new CCMultiChannelBuffer(channelCount, readBufferSize);
			// make sure the out buffer is the correct size and type.
			outBuffer.setChannelCount(channelCount);
			// how many samples to read total
			long totalSampleCount = stream.getSampleFrameLength();
			if (totalSampleCount == -1) {
				totalSampleCount = AudioUtils.millis2Frames(stream.getMillisecondLength(), stream.getFormat());
			}
			debug("Total sample count for " + filename + " is " + totalSampleCount);
			outBuffer.setBufferSize((int) totalSampleCount);

			// now read in chunks.
			long totalSamplesRead = 0;
			while (totalSamplesRead < totalSampleCount) {
				// is the remainder smaller than our buffer?
				if (totalSampleCount - totalSamplesRead < readBufferSize) {
					readBuffer.setBufferSize((int) (totalSampleCount - totalSamplesRead));
				}

				int samplesRead = stream.read(readBuffer);

				if (samplesRead == 0) {
					debug("loadSampleIntoBuffer: got 0 samples read");
					break;
				}

				// copy data from one buffer to the other.
				for (int i = 0; i < channelCount; ++i) {
					// a faster way to do this would be nice.
					for (int s = 0; s < samplesRead; ++s) {
						outBuffer.setSample(i, (int) totalSamplesRead + s, readBuffer.getSample(i, s));
					}
				}

				totalSamplesRead += samplesRead;
			}

			if (totalSamplesRead != totalSampleCount) {
				outBuffer.setBufferSize((int) totalSamplesRead);
			}

			debug("loadSampleIntoBuffer: final output buffer size is " + outBuffer.getBufferSize());

			stream.close();
		} else {
			debug("Unable to load an CCAudioRecordingStream for " + filename);
		}

		return sampleRate;
	}

	/**
	 * Creates an AudioRecorder that will use the provided Recordable object as
	 * its record source and that will save to the file name specified.
	 * Recordable classes in Minim include AudioOutput, AudioInput, AudioPlayer,
	 * and AudioSample. The format of the file will be inferred from the
	 * extension in the file name. If the extension is not a recognized file
	 * type, this will return null.
	 * 
	 * @param source the <code>Recordable</code> object you want to use as a
	 *            record source
	 * @param fileName the name of the file to record to
	 * 
	 * @return an <code>AudioRecorder</code> for the record source
	 */

	static public CCAudioRecorder createRecorder(CCRecordable source, String fileName) {
		return createRecorder(source, fileName, false);
	}
	
	/**
	 * Should return a {@link CCSampleRecorder} that can record the
	 * <code>source</code> in a buffered (in-memory) or non-buffered (streamed)
	 * manner, to the file specified by <code>saveTo</code>
	 * 
	 * @param source the audio source that should be recorded
	 * @param saveTo the file to save the recorded audio to
	 * @param buffered whether or not to buffer all recorded audio in memory or
	 *            stream directly to the file
	 * @return an appropriate SampleRecorder
	 */
	static public CCSampleRecorder getSampleRecorder(CCRecordable source, String fileName, boolean buffered) {
		// do nothing if we can't generate a place to put the file

		String ext = fileName.substring(fileName.lastIndexOf('.') + 1).toLowerCase();
		debug("createRecorder: file extension is " + ext + ".");
		AudioFileFormat.Type fileType = CCSoundFormat.getFormat(ext).type();
		

		CCSampleRecorder recorder = null;

		Path destPath = CCNIOUtil.dataPath(fileName);
		if (buffered) {
			recorder = new CCBufferedSampleRecorder(destPath, fileType, source.getFormat(), source.bufferSize());
		} else {
			recorder = new CCStreamingSampleRecorder(destPath, fileType, source.getFormat(), source.bufferSize());
		}

		return recorder;
	}

	/**
	 * Creates an AudioRecorder that will use the provided Recordable object as
	 * its record source and that will save to the file name specified.
	 * Recordable classes in Minim include AudioOutput, AudioInput, AudioPlayer,
	 * and AudioSample. The format of the file will be inferred from the
	 * extension in the file name. If the extension is not a recognized file
	 * type, this will return null. Be aware that if you choose buffered
	 * recording the call to AudioRecorder's save method will block until the
	 * entire buffer is written to disk. In the event that the buffer is very
	 * large, your app will noticeably hang.
	 * 
	 * @param source the <code>Recordable</code> object you want to use as a
	 *            record source
	 * @param fileName the name of the file to record to
	 * @param buffered boolean: whether or not to use buffered recording
	 * 
	 * @return an <code>AudioRecorder</code> for the record source
	 */
	static public CCAudioRecorder createRecorder(CCRecordable source, String fileName, boolean buffered) {
		CCSampleRecorder rec = getSampleRecorder(source, fileName, buffered);
		if (rec != null) {
			return new CCAudioRecorder(source, rec);
		}
		
		throw new CCSoundException("Couldn't create an AudioRecorder for " + fileName + ".");
	}

	/**
	 * An AudioInput is used when you want to monitor the active audio input of
	 * the computer. On a laptop, for instance, this will typically be the
	 * built-in microphone. On a desktop it might be the line-in port on the
	 * soundcard. The default values are for a stereo input with a 1024 sample
	 * buffer (ie the size of left, right, and mix buffers), sample rate of
	 * 44100 and bit depth of 16. Generally speaking, you will not want to
	 * specify these things, but it's there if you need it.
	 * 
	 * @return an AudioInput that reads from the active audio input of the
	 *         soundcard
	 * 
	 * @see #getLineIn(int, int, float, int)
	 */
	public static CCLineIn getLineIn() {
		return getLineIn(STEREO);
	}

	/**
	 * Gets either a MONO or STEREO {@link CCLineIn}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @return an <code>AudioInput</code> with the requested type, a 1024 sample
	 *         buffer, a sample rate of 44100 and a bit depth of 16
	 * @see #getLineIn(int, int, float, int)
	 */
	public static CCLineIn getLineIn(int type) {
		return getLineIn(type, 1024, 44100, 16);
	}

	/**
	 * Gets an {@link CCLineIn}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the <code>AudioInput</code>'s
	 *            sample buffer to be (ie the size of left, right, and mix
	 *            buffers)
	 * @return an <code>AudioInput</code> with the requested attributes, a
	 *         sample rate of 44100 and a bit depth of 16
	 * @see #getLineIn(int, int, float, int)
	 */
	public static CCLineIn getLineIn(int type, int bufferSize) {
		return getLineIn(type, bufferSize, 44100, 16);
	}

	/**
	 * Gets an {@link CCLineIn}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the <code>AudioInput</code>'s
	 *            sample buffer to be (ie the size of left, right, and mix
	 *            buffers)
	 * @param sampleRate float: the desired sample rate in Hertz (typically
	 *            44100)
	 * @return an <code>AudioInput</code> with the requested attributes and a
	 *         bit depth of 16
	 * @see #getLineIn(int, int, float, int)
	 */
	public static CCLineIn getLineIn(int type, int bufferSize, float sampleRate) {
		return getLineIn(type, bufferSize, sampleRate, 16);
	}
	
	static TargetDataLine getTargetDataLine(AudioFormat format, int bufferSize) {
		TargetDataLine line = null;
		DataLine.Info info = new DataLine.Info(TargetDataLine.class, format);
		if (AudioSystem.isLineSupported(info)) {
			try {
				if (inputMixer == null) {
					line = (TargetDataLine) AudioSystem.getLine(info);
				} else {
					line = (TargetDataLine) inputMixer.getLine(info);
				}
				line.open(format, bufferSize * format.getFrameSize());
				debug("TargetDataLine buffer size is " + line.getBufferSize() + "\n" + "TargetDataLine format is "
						+ line.getFormat().toString() + "\n" + "TargetDataLine info is "
						+ line.getLineInfo().toString());
			} catch (Exception e) {
				throw new CCSoundException("Error acquiring TargetDataLine", e);
			}
		} else {
			throw new CCSoundException("Unable to return a TargetDataLine: unsupported format - " + format.toString());
		}
		return line;
	}
	
	/**
	 * Should return an {@link CCAudioStream} with the requested parameters. What
	 * Minim is expecting this stream to be reading from is the active audio
	 * input of the computer, such as the microphone or line-in.
	 * 
	 * @param type Minim.STEREO or Minim.MONO
	 * @param bufferSize how big the in-memory buffer should be
	 * @param sampleRate what the sample rate of the stream should be
	 * @param bitDepth what the bit depth of the stream should be
	 * @return an AudioStream that is reading from the active audio input of the
	 *         computer
	 */
	public static CCAudioStream getAudioInput(int type, int bufferSize, float sampleRate, int bitDepth) {
		if (bitDepth != 8 && bitDepth != 16) {
			throw new IllegalArgumentException("Unsupported bit depth, use either 8 or 16.");
		}
		AudioFormat format = new AudioFormat(sampleRate, bitDepth, type, true, false);
		TargetDataLine line = getTargetDataLine(format, bufferSize * 4);
		if (line != null) {
			return new CCAudioInput(line, bufferSize);
		}
		return null;
	}

	/**
	 * Gets an {@link CCLineIn}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the <code>AudioInput</code>'s
	 *            sample buffer to be (ie the size of left, right, and mix
	 *            buffers)
	 * @param sampleRate float: the desired sample rate in Hertz (typically
	 *            44100)
	 * @param bitDepth int: the desired bit depth (typically 16)
	 * @return an <code>AudioInput</code> with the requested attributes
	 */
	public static CCLineIn getLineIn(int type, int bufferSize, float sampleRate, int bitDepth) {
		CCLineIn input = null;
		CCAudioStream stream = getAudioInput(type, bufferSize, sampleRate, bitDepth);
		if (stream != null) {
			CCAudioOutput out = getAudioOutput(type, bufferSize, sampleRate, bitDepth);
			if (out != null) {
				input = new CCLineIn(stream, out);
			} else {
				stream.close();
			}
		}

		if (input != null) {
			addSource(input);
		} else {
			throw new CCSoundException("Minim.getLineIn: attempt failed, could not secure an AudioInput.");
		}

		return input;
	}

	/**
	 * Get the input as an AudioStream that you can read from yourself, rather
	 * than wrapped in an AudioInput that does that work for you.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the AudioStream's interal buffer
	 *            to be.
	 * @param sampleRate float: the desired sample rate in Hertz (typically
	 *            44100)
	 * @param bitDepth int: the desired bit depth (typically 16)
	 * @return an AudioStream that reads from the input source of the soundcard.
	 */
	public static CCAudioStream getInputStream(int type, int bufferSize, float sampleRate, int bitDepth) {
		CCAudioStream stream = getAudioInput(type, bufferSize, sampleRate, bitDepth);
		streams.add(stream);
		return stream;
	}

	/**
	 * An AudioOutput is used to generate sound in real-time and output it to
	 * the soundcard. Usually, the sound generated by an AudioOutput will be
	 * heard through the speakers or headphones attached to a computer. The
	 * default parameters for an AudioOutput are STEREO sound, a 1024 sample
	 * buffer (ie the size of the left, right, and mix buffers), a sample rate
	 * of 44100, and a bit depth of 16. To actually generate sound with an
	 * AudioOutput you need to patch at least one sound generating UGen to it,
	 * such as an Oscil.
	 * <p>
	 * Using setOutputMixer you can also create AudioOutputs that send sound to
	 * specific output channels of a soundcard.
	 * 
	 * @return an AudioOutput that can be used to generate audio
	 * @see #getLineOut(int, int, float, int)
	 * @related AudioOutput
	 * @related UGen
	 */
	public static CCAudioOutput getLineOut() {
		return getLineOut(STEREO);
	}

	/**
	 * Gets an {@link AudioOutput}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @return an <code>AudioOutput</code> with the requested type, a 1024
	 *         sample buffer, a sample rate of 44100 and a bit depth of 16
	 * @see #getLineOut(int, int, float, int)
	 */
	public static CCAudioOutput getLineOut(int type) {
		return getLineOut(type, 1024, 44100, 16);
	}

	/**
	 * Gets an {@link AudioOutput}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the AudioOutput's sample buffer
	 *            to be (ie the size of the left, right, and mix buffers)
	 * @return an <code>AudioOutput</code> with the requested attributes, a
	 *         sample rate of 44100 and a bit depth of 16
	 * @see #getLineOut(int, int, float, int)
	 */
	public static CCAudioOutput getLineOut(int type, int bufferSize) {
		return getLineOut(type, bufferSize, 44100, 16);
	}

	/**
	 * Gets an {@link AudioOutput}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the AudioOutput's sample buffer
	 *            to be (ie the size of the left, right, and mix buffers)
	 * @param sampleRate float: the desired sample rate in Hertz (typically
	 *            44100)
	 * @return an <code>AudioOutput</code> with the requested attributes and a
	 *         bit depth of 16
	 * @see #getLineOut(int, int, float, int)
	 */
	public static CCAudioOutput getLineOut(int type, int bufferSize, float sampleRate) {
		return getLineOut(type, bufferSize, sampleRate, 16);
	}

	/**
	 * Gets an {@link AudioOutput}.
	 * 
	 * @param type Minim.MONO or Minim.STEREO
	 * @param bufferSize int: how long you want the AudioOutput's sample buffer
	 *            to be (ie the size of the left, right, and mix buffers)
	 * @param sampleRate float: the desired sample rate in Hertz (typically
	 *            44100)
	 * @param bitDepth int: the desired bit depth (typically 16)
	 * @return an <code>AudioOutput</code> with the requested attributes
	 */
	public static CCAudioOutput getLineOut(int type, int bufferSize, float sampleRate, int bitDepth) {
		CCAudioOutput out = getAudioOutput(type, bufferSize, sampleRate, bitDepth);
		if (out != null) {
			addSource(out);
			return out;
		}

		throw new CCSoundException("Minim.getLineOut: attempt failed, could not secure a LineOut.");
	}
}
